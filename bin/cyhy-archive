#!/usr/bin/env python
"""Archive CyHy data to file and delete archived data from the CyHy database.

Usage:
  cyhy-archive [options] ARCHIVE_DIR
  cyhy-archive (-h | --help)
  cyhy-archive --version

Options:
  -d --debug                     Enable debug logging
  -h --help                      Show this screen.
  -n --no-commander-pause        Do not attempt to pause the CyHy commander
  --version                      Show version.
  -s SECTION --section=SECTION   Configuration section to use.
"""

import datetime
import logging
import os
import re
import subprocess
import sys
import time

from bson import ObjectId
from docopt import docopt

from cyhy.core.config import Config
from cyhy.db import database, CHDatabase
from cyhy.util import util

try:
    from urlparse import urlparse
except ImportError:
    from urllib.parse import urlparse

LOGGER = logging.getLogger("cyhy-archive")
LOG_FILE = "/var/log/cyhy/archive.log"
DEFAULT_LOGGER_LEVEL = logging.INFO
COLLECTIONS_TO_ARCHIVE = [
    {"name": "HostScanDoc", "age_limit_days": 548},
    {"name": "PortScanDoc", "age_limit_days": 90},
    {"name": "VulnScanDoc", "age_limit_days": 548},
]


def logging_setup(debug_logging):
    """Configure logging."""
    if debug_logging:
        level = logging.DEBUG
    else:
        level = DEFAULT_LOGGER_LEVEL
    util.setup_logging(level, filename=LOG_FILE)
    LOGGER.debug(
        "Debug logging enabled"
    )  # Only output if debug is enabled.  Skipped otherwise.


def get_db_info_from_config(config_section=None):
    """Retrieve and parse database URI from the configuration file."""
    # NOTE: ConfigParser will choke if special characters ('%' or '?')
    # appear in the URI
    config = Config(config_section)
    # encode special character '#' that could be in a password, otherwise
    # urlparse will parse incorrectly
    # ASSUMPTION: '#' is only ever going to appear in the password portion
    # of the URI
    parsed_db_uri = urlparse(config.db_uri.replace("#", "%23"))
    return parsed_db_uri


def pause_commander(mongo_db):
    """Pause the commander and return the pause document id."""
    # number of iterations to wait before giving up
    pause_iteration_limit = 60
    # number of seconds to wait between each check to see if the commander
    # has paused
    pause_iteration_wait_seconds = 30
    pause_iteration_count = 0

    cyhy_db = CHDatabase(mongo_db)
    doc = cyhy_db.pause_commander("cyhy-archive", "archive in progress")
    if not doc.get("_id"):
        LOGGER.error(
            "Commander pause control document _id not found! Is the database up?"
        )
        return None
    LOGGER.info("Requesting commander pause (control doc id = %s)", doc["_id"])
    while not doc["completed"]:
        pause_iteration_count += 1
        LOGGER.info("  Waiting for commander to pause... (#%d)", pause_iteration_count)
        time.sleep(pause_iteration_wait_seconds)
        if pause_iteration_count == pause_iteration_limit:
            LOGGER.warning(
                "Commander failed to pause! "
                "It may not be running, running in a long cycle, or hung."
            )
            return doc["_id"]
        doc.reload()
    return doc["_id"]


def resume_commander(mongo_db, pause_doc_id):
    """Unpause the commander by removing the pause document."""
    doc = mongo_db.SystemControlDoc.find_one({"_id": ObjectId(pause_doc_id)})
    if not doc:
        LOGGER.error("Could not find a control doc with id %s", pause_doc_id)
        return False
    doc.delete()
    LOGGER.info(
        (
            "Commander control doc %s successfully deleted "
            "(commander should resume unless other control docs exist)"
        ),
        pause_doc_id,
    )
    return True


def archive_and_delete(mongo_db, curr_date, archive_dir, parsed_db_uri):
    """Archive old documents to disk and remove them from the database."""
    doc_counts = {"pre-archiving": dict(), "post-archiving": dict()}
    results = {
        "export_success": False,
        "exported_counts": dict(),
        "delete_success": False,
        "deleted_counts": dict(),
    }
    today_str = curr_date.strftime("%Y%m%d")

    for collection_info in COLLECTIONS_TO_ARCHIVE:
        # e.g. db_collection == db['HostScanDoc'] == db.HostScanDoc
        db_collection = mongo_db[collection_info["name"]]
        # e.g. collection_name == 'host_scans'
        collection_name = db_collection.collection.name

        # count documents in collection before archiving
        doc_counts["pre-archiving"][collection_name] = db_collection.find({}).count()
        LOGGER.info(
            "%s: %s documents exist before archiving",
            collection_name,
            "{:,}".format(doc_counts["pre-archiving"][collection_name]),
        )

        cutoff_date = curr_date - datetime.timedelta(
            days=collection_info["age_limit_days"]
        )
        # check to see if at least one document is eligible to be archived
        if db_collection.find_one({"latest": False, "time": {"$lt": cutoff_date}}):
            # use mongodump to create an archive for this collection
            archive_file = "{}/cyhy_archive_{}_{}.gz".format(
                archive_dir, collection_name, today_str
            )
            query = "{{latest:false, time:{{$lt:{}}}}}".format(
                cutoff_date.strftime('ISODate("%Y-%m-%dT%H:%M:%S.%fZ")')
            )

            if parsed_db_uri.port:
                db_host_port = "{}:{}".format(
                    parsed_db_uri.hostname, parsed_db_uri.port
                )
            else:
                db_host_port = parsed_db_uri.hostname

            mongodump_command = [
                "mongodump",
                "-v",
                "--gzip",
                "--archive={}".format(archive_file),
                "--host={}".format(db_host_port),
                "--db={}".format(mongo_db.name),
                "--collection={}".format(collection_name),
                "--query={}".format(query),
            ]

            if parsed_db_uri.username:
                # unencode special characters in password
                # (see get_db_info_from_config)
                mongodump_command += [
                    "--username={}".format(parsed_db_uri.username),
                    "--password={}".format(parsed_db_uri.password.replace("%23", "#")),
                ]

            auth_db = parsed_db_uri.path.lstrip("/")
            if auth_db:
                mongodump_command += ["--authenticationDatabase={}".format(auth_db)]

            proc = subprocess.Popen(
                mongodump_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE,
            )
            LOGGER.info(
                "%s: exporting documents older than %s to %s",
                collection_name,
                cutoff_date.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
                archive_file,
            )
            stdout_data, stderr_data = proc.communicate()
            if proc.returncode != 0:
                LOGGER.error(
                    "%s: mongodump failed with return code %s",
                    collection_name,
                    proc.returncode,
                )
                LOGGER.debug("mongodump command:\n%s", " ".join(mongodump_command))
                LOGGER.debug("mongodump output:\n%s", stdout_data)
                LOGGER.error("mongodump error:\n%s", stderr_data)
                return results

            # check that at least one document was exported
            regex = re.compile(r"\d+ document")
            results["exported_counts"][collection_name] = int(
                regex.findall(stderr_data)[0].split(" ")[0]
            )
            if results["exported_counts"][collection_name] < 1:
                LOGGER.error("%s: no documents exported", collection_name)
                return results
            results["export_success"] = True
            LOGGER.info(
                "%s: %s documents successfully exported",
                collection_name,
                "{:,}".format(results["exported_counts"][collection_name]),
            )

            # delete documents in the database, now that we have confirmed
            # successful export to archive
            delete_output = db_collection.collection.remove(
                {"latest": False, "time": {"$lt": cutoff_date}}
            )
            results["deleted_counts"][collection_name] = delete_output.get("n")

            # check that number of deleted documents matches the number of
            # exported/archived documents
            if (
                results["deleted_counts"][collection_name]
                != results["exported_counts"][collection_name]
            ):
                LOGGER.error(
                    (
                        "%s: count of exported documents (%s) does "
                        "not match count of deleted documents (%s)"
                    ),
                    collection_name,
                    "{:,}".format(results["exported_counts"][collection_name]),
                    "{:,}".format(results["deleted_counts"][collection_name]),
                )
                return results
            results["delete_success"] = True
            LOGGER.info(
                "%s: %s documents successfully deleted",
                collection_name,
                "{:,}".format(results["deleted_counts"][collection_name]),
            )

            # count documents in collection after archiving
            doc_counts["post-archiving"][collection_name] = db_collection.find(
                {}
            ).count()
            LOGGER.info(
                "%s: %s documents exist after archiving",
                collection_name,
                "{:,}".format(doc_counts["post-archiving"][collection_name],),
            )
        else:  # no documents to archive
            LOGGER.warning(
                ("%s: no documents are old enough to be archived (cutoff date: %s)"),
                collection_name,
                cutoff_date.strftime("%Y-%m-%dT%H:%M:%S.%fZ"),
            )
            results["exported_counts"][collection_name] = results["deleted_counts"][
                collection_name
            ] = 0
            results["export_success"] = results["delete_success"] = True
    return results


def main():
    """Run the archive function and log the results."""
    start_time = util.utcnow()
    args = docopt(__doc__, version="v0.0.1")
    logging_setup(args["--debug"])
    LOGGER.info("cyhy-archive started")
    mongo_db = database.db_from_config(args["--section"])
    # creds needed in subprocess call to mongodump
    parsed_db_uri = get_db_info_from_config(args["--section"])
    if parsed_db_uri.port:
        db_host_port_name = "{}:{}/{}".format(
            parsed_db_uri.hostname, parsed_db_uri.port, mongo_db.name
        )
    else:
        db_host_port_name = "{}/{}".format(parsed_db_uri.hostname, mongo_db.name)

    archive_directory = args["ARCHIVE_DIR"]
    LOGGER.info("=" * 60)
    LOGGER.info("Database to be archived: %s", db_host_port_name)
    LOGGER.info("Directory to archive to: %s", archive_directory)
    if not os.path.exists(archive_directory):
        LOGGER.info("  %s does not exist - creating!", archive_directory)
        os.makedirs(archive_directory)
    LOGGER.info("Data to be archived:")
    for collection_info in COLLECTIONS_TO_ARCHIVE:
        LOGGER.info(
            "  %s older than %s days",
            mongo_db[collection_info["name"]].collection.name,
            "{:,}".format(collection_info["age_limit_days"]),
        )
    LOGGER.info("=" * 60)

    if not args["--no-commander-pause"]:
        commander_pause_id = pause_commander(mongo_db)
        if not commander_pause_id:
            LOGGER.fatal("Exiting; no data was archived!")
            sys.exit(-1)

    results = archive_and_delete(mongo_db, start_time, archive_directory, parsed_db_uri)
    if not results["export_success"]:
        LOGGER.fatal("Exiting; data export failed, no data was deleted from database!")
        LOGGER.fatal("Clean up any exported data in %s", archive_directory)
        for collection_info in COLLECTIONS_TO_ARCHIVE:
            collection_name = mongo_db[collection_info["name"]].collection.name
            LOGGER.fatal(
                "  %s: exported %s documents",
                collection_name,
                "{:,}".format(results["exported_counts"].get(collection_name, 0)),
            )
        if not args["--no-commander-pause"]:
            if not resume_commander(
                mongo_db, commander_pause_id
            ):  # start commander back up
                LOGGER.error("Exiting abnormally- verify that commander has resumed!")
        sys.exit(-1)

    if not results["delete_success"]:
        LOGGER.fatal(
            "Exiting; data delete failed, but some data may have "
            "been successfully exported, then deleted from the db:"
        )
        for collection_info in COLLECTIONS_TO_ARCHIVE:
            collection_name = mongo_db[collection_info["name"]].collection.name
            LOGGER.fatal(
                "  %s: exported %s documents",
                collection_name,
                "{:,}".format(results["exported_counts"].get(collection_name, 0)),
            )
            LOGGER.fatal(
                "  %s: deleted %s documents",
                collection_name,
                "{:,}".format(results["deleted_counts"].get(collection_name, 0)),
            )

    if not args["--no-commander-pause"]:
        if not resume_commander(mongo_db, commander_pause_id):
            LOGGER.error("Exiting abnormally- verify that commander has resumed!")
            sys.exit(-1)

    duration = util.utcnow() - start_time
    LOGGER.info(
        "cyhy-archive successfully completed in %s seconds (%s minutes)",
        "{:0,.3f}".format(duration.seconds + (duration.microseconds / 1000000.0)),
        "{:0.1f}".format(duration.seconds / 60.0),
    )
    # import IPython; IPython.embed() #<<< BREAKPOINT >>>


if __name__ == "__main__":
    main()
